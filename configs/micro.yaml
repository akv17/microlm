dataset:
  path: "assets/data/corpus100k.txt"
  val_size: 10000

tokenizer:
  seq_len: 64

model:
  dim: 512
  ffn: 1024
  heads: 8
  blocks: 6
  dropout: 0.1
  device: "cpu"

train:
  dst: "assets/train/1m"
  device: "cpu"
  batch_size: 8
  epochs: 10
  workers: 0
  log_every: 1
  eval_every: 100

generate:
  device: "cpu"
  max_size: 128
